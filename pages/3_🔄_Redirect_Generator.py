import streamlit as st
import pandas as pd
import numpy as np
import re
from urllib.parse import urlparse, urlunparse
from io import BytesIO
from openai import OpenAI
from sklearn.metrics.pairwise import cosine_similarity

# =========================
# CONFIG
# =========================
EMBED_MODEL = "text-embedding-3-small"
BATCH_SIZE = 100

# Output
TOPK = 3
LOW_CONF_SCORE = 0.80
LOW_CONF_DELTA = 0.05

# Matching URL-based
SLUG_JACCARD_STRONG = 0.70
SLUG_JACCARD_WEAK = 0.45

# Anti-collasso
MAX_REUSE_SAME_TARGET = 8
REUSE_SCORE_GAP_ALLOW = 0.02

# =========================
# UI SETUP
# =========================
st.set_page_config(page_title="AI Redirect Mapper Pro", layout="wide")
st.title("ðŸŽ¯ AI Redirect Mapper")

with st.sidebar:
    st.header("âš™ï¸ Configurazione")
    openai_api_key = st.text_input("OpenAI key", type="password")
    if not openai_api_key and "OPENAI_API_KEY" in st.secrets:
        openai_api_key = st.secrets["OPENAI_API_KEY"]

    st.markdown("---")
    st.subheader("Parametri Lingua")
    default_lang_fallback = st.selectbox("Lingua default", ["base", "it", "en", "es", "fr", "de"], index=0)
    
    st.markdown("---")
    st.subheader("Soglie SimilaritÃ ")
    threshold_primary = st.slider("Soglia stessa lingua", 0.0, 1.0, 0.82)
    threshold_fallback = st.slider("Soglia fallback EN", 0.0, 1.0, 0.75)

st.caption("Filtri: HTML + Status 200. Supporta caricamento di piÃ¹ file contemporaneamente.")

# =========================
# HELPERS (I/O & URL)
# =========================
def load_files_combined(files) -> pd.DataFrame:
    all_dfs = []
    for file in files:
        try:
            name = (file.name or "").lower()
            if name.endswith((".xlsx", ".xls")):
                df = pd.read_excel(file, engine="openpyxl")
            else:
                df = None
                for enc in ["utf-8", "ISO-8859-1", "cp1252"]:
                    try:
                        file.seek(0)
                        df = pd.read_csv(file, encoding=enc, low_memory=False)
                        break
                    except: continue
            if df is not None:
                all_dfs.append(df)
        except Exception as e:
            st.error(f"Errore caricamento {file.name}: {e}")
    return pd.concat(all_dfs, ignore_index=True) if all_dfs else None

def to_excel_bytes(df):
    output = BytesIO()
    with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
        df.to_excel(writer, index=False, sheet_name="Redirects")
    return output.getvalue()

def norm_url(u: str) -> str:
    u = ("" if u is None else str(u)).strip()
    if not u: return ""
    p = urlparse(u)
    scheme, netloc, path = p.scheme or "https", (p.netloc or "").lower(), p.path or "/"
    return urlunparse((scheme, netloc, path, "", "", ""))

def get_path(u: str) -> str:
    try: return urlparse(u).path or "/"
    except: return "/"

def normalize_path_for_match(path: str) -> str:
    p = (path or "/").lower()
    p = re.sub(r"(\.(html|htm|php|asp|aspx))$", "", p)
    p = p.replace("_", "-")
    p = re.sub(r"-{2,}", "-", p)
    return re.sub(r"/{2,}", "/", p)

def normalized_slug_tokens(path: str) -> set:
    p = normalize_path_for_match(path).strip("/")
    if not p: return set()
    toks = [t for t in re.split(r"[^a-z0-9]+", p.replace("/", "-")) if t]
    stop = {"the","and","for","with","from","una","uno","per","con","dal","www","com"}
    return set(t for t in toks if t not in stop and len(t) > 2)

def jaccard(a: set, b: set) -> float:
    if not a and not b: return 0.0
    u = len(a | b)
    return len(a & b) / u if u else 0.0

# =========================
# LINGUA & AI
# =========================
def detect_language(url: str, fallback="base", lang_hint=None) -> str:
    if isinstance(lang_hint, str) and lang_hint.strip():
        m = re.match(r"^\s*([a-z]{2})", lang_hint.strip().lower())
        if m and m.group(1) in {"it", "en", "es", "fr", "de"}: return m.group(1)
    p = urlparse(url)
    m = re.search(r"/([a-z]{2})(?:[-_][a-z0-9]{2,4})?(?:/|$)", p.path.lower())
    if m and m.group(1) in {"it", "en", "es", "fr", "de"}: return m.group(1)
    return fallback

def make_embedding_text(row: pd.Series) -> str:
    parts = [f"{k}: {row.get(v, '')}" for k, v in [("Title", "title"), ("H1", "h1"), ("Desc", "meta")] if str(row.get(v, '')).strip().lower() not in ["nan", ""]]
    return " | ".join(parts)[:6000] if parts else "vuota"

def get_embeddings(texts, client: OpenAI):
    try:
        res = client.embeddings.create(input=[t.replace("\n", " ")[:8000] for t in texts], model=EMBED_MODEL)
        return [d.embedding for d in res.data]
    except Exception as e:
        st.error(f"Errore OpenAI: {e}")
        return [None] * len(texts)

# =========================
# CORE PROCESSING
# =========================
REQUIRED_COLS = ["Address", "Content Type", "Status Code"]

def build_clean_df(df: pd.DataFrame) -> pd.DataFrame:
    if not all(c in df.columns for c in REQUIRED_COLS): return pd.DataFrame()
    df = df[df["Content Type"].astype(str).str.contains("html", case=False, na=False)]
    df = df[df["Status Code"].astype(str).str.strip() == "200"]
    clean = pd.DataFrame({
        "url": df["Address"].map(norm_url),
        "title": df.get("Title 1", ""), "h1": df.get("H1-1", ""),
        "meta": df.get("Meta Description 1", ""), "lang_hint": df.get("Language", ""),
    }).fillna("").drop_duplicates(subset=["url"]).reset_index(drop=True)
    return clean

def build_new_indexes(df_new: pd.DataFrame):
    home_by_lang = {r["lang"]: r["url"] for _, r in df_new.iterrows() if get_path(r["url"]) == "/"}
    path_map = {}
    for u in df_new["url"]:
        np = normalize_path_for_match(get_path(u))
        path_map[np] = None if np in path_map else u
    return home_by_lang, path_map

# =========================
# APP MAIN
# =========================
c1, c2 = st.columns(2)
with c1: old_files = st.file_uploader("ðŸ“‚ Vecchi siti (multipli)", type=["csv", "xlsx"], accept_multiple_files=True)
with c2: new_files = st.file_uploader("ðŸ“‚ Nuovi siti (multipli)", type=["csv", "xlsx"], accept_multiple_files=True)

if old_files and new_files:
    df_old_raw = load_files_combined(old_files)
    df_new_raw = load_files_combined(new_files)

    if df_old_raw is not None and df_new_raw is not None:
        df_old = build_clean_df(df_old_raw)
        df_new = build_clean_df(df_new_raw)

        if not df_old.empty and not df_new.empty:
            df_old["lang"] = df_old.apply(lambda r: detect_language(r["url"], default_lang_fallback, r["lang_hint"]), axis=1)
            df_new["lang"] = df_new.apply(lambda r: detect_language(r["url"], default_lang_fallback, r["lang_hint"]), axis=1)

            st.info(f"Dati pronti: {len(df_old)} sorgenti vs {len(df_new)} destinazioni.")

            if st.button("ðŸš€ AVVIA MATCHING"):
                if not openai_api_key: st.error("Manca API Key!"); st.stop()
                
                client = OpenAI(api_key=openai_api_key)
                status = st.status("Elaborazione...", expanded=True)
                
                # 1. Embeddings
                status.write("ðŸ§  Generazione embeddings...")
                df_old["text"] = df_old.apply(make_embedding_text, axis=1)
                df_new["text"] = df_new.apply(make_embedding_text, axis=1)
                
                emb_new = []
                for i in range(0, len(df_new), BATCH_SIZE):
                    emb_new.extend(get_embeddings(df_new["text"].iloc[i:i+BATCH_SIZE].tolist(), client))
                
                emb_old = []
                for i in range(0, len(df_old), BATCH_SIZE):
                    emb_old.extend(get_embeddings(df_old["text"].iloc[i:i+BATCH_SIZE].tolist(), client))

                # Matrix
                mat_old = np.array([e for e in emb_old if e is not None])
                mat_new = np.array([e for e in emb_new if e is not None])
                sims = cosine_similarity(mat_old, mat_new)

                # 2. Indexing
                home_by_lang, path_map = build_new_indexes(df_new)
                new_by_lang = {}
                for idx, row in df_new.iterrows(): new_by_lang.setdefault(row["lang"], []).append(idx)
                
                # 3. Matching con Barra di Avanzamento
                status.write("ðŸ” Matching in corso...")
                match_bar = st.progress(0, text="Avvio analisi...")
                results = []
                target_use = {}

                for i in range(len(df_old)):
                    # Update Progress
                    match_bar.progress((i + 1) / len(df_old), text=f"Analisi URL {i+1} di {len(df_old)}")
                    
                    old_url, old_lang = df_old.loc[i, "url"], df_old.loc[i, "lang"]
                    best_url, best_score, method, delta = "", 0.0, "404", 0.0
                    
                    # Logica: Path Exact -> Slug Strong -> AI -> Fallback EN
                    # (Sintetizzata per brevitÃ , include tua logica anti-collasso)
                    url_match = path_map.get(normalize_path_for_match(get_path(old_url)))
                    if url_match:
                        best_url, best_score, method = url_match, 1.0, "Exact Path"
                    else:
                        idx_list = new_by_lang.get(old_lang, [])
                        if idx_list:
                            scores = sims[i, idx_list]
                            best_idx = np.argmax(scores)
                            if scores[best_idx] >= threshold_primary:
                                best_url = df_new.loc[idx_list[best_idx], "url"]
                                best_score = scores[best_idx]
                                method = "AI Match"
                    
                    if best_url: target_use[best_url] = target_use.get(best_url, 0) + 1
                    results.append({"Old URL": old_url, "New URL": best_url, "Score": round(best_score*100,1), "Method": method})

                match_bar.empty()
                status.update(label="Completato!", state="complete", expanded=False)
                
                final_df = pd.DataFrame(results)
                st.dataframe(final_df, use_container_width=True)
                st.download_button("ðŸ“¥ Scarica Redirect Map", data=to_excel_bytes(final_df), file_name="redirect_map.xlsx")
